"""This file contains SearchEngine, a class that implements graph algorithms
to perform search engine functionalities.
"""
import os # file io
import pickle # file io
from multiprocessing import Pool, cpu_count
from io import FileIO
from time import time


from src.minimum_heap.min_heap import MinHeap
from src.graphs.search_graph import SearchGraph
from src.graphs.search_item import SearchItem
from src.graphs.search_algorithms import dijkstra
from src.utils.formatting import wordtrie_format
from src.utils.similarities import name_similarity


class SearchEngine(SearchGraph):
    """Uses graph algorithms on SearchGraph to implement a search engine.
    
    SearchEngine inherits SearchGraph.
    
    Attributes:
        graph: An adjacency matrix implemented using 2-dimensional lists.
        items: A list of SearchItems.
        words: A WordTrie containing all the SearchItem names.
        size: The active size of the adjacency matrix.
    """
    def __init__(self, init_file: str=None, precomputed_path: str=None) -> None:
        """Constructs a SearchEngine.
        
        Args:
            init_file: A str filepath to init a SearchGraph from. This file 
                should be a file generated by SearchGraph's save_instance().
            precomputed_path: A str path to a directory of precomputed 
                recommendations.
        Returns:
            None.
        """
        super().__init__(init_file)
        if precomputed_path:
            if len(os.listdir(precomputed_path)) >= len(self.items):
                self.pc_path = precomputed_path
            else:
                print('[ERROR] SearchEngine(): '
                    'Please use the correct precomputed path or generate '
                    'a new one using save_all_recommends().')
            
    async def search(self, query: str, 
                     limit: int=100) -> list[SearchItem]:
        """An awaitable function to search from from a str query,
        Returns a list of SearchItems.
        
        Args:
            query: A str query to search for.
            limit: An (optional) int results limit. Defaults to 100. 
            
        Returns:
            A list of query-matching SearchItems.
        """
        query = wordtrie_format(query)
        query = self.words.word_suggestions(query)
        results = []
        for item_name in query:
            if len(results) >= limit:
                break
            # search by tag
            if item_name in self.tag_dict:
                # get tag index
                tag_index = self.tag_dict[item_name]
                for item_index in self.tag_item[tag_index]:
                    if len(results) >= limit:
                        break
                    self.add_appearance(item_index)
                    results.append(item_index)
            # search by name
            else:
                # update appearance count
                item_index = self.get_item_index(item_name)
                self.add_appearance(item_index)
                results.append(item_index)
        return self._extend_results(results, item_index, limit)
    
    async def recommend(self, query: str, 
                        limit: int=100) -> list[SearchItem]:
        """An awaitable function, returns a list of recommended 
        SearchItems from a given search query.
        
        Args:
            query: A str query to recommend from.
            limit: An (optional) int results limit. Defaults to 100. 
        
        Returns: 
            A list of recommended SearchItems.
        """
        query = wordtrie_format(query)
        query = self.words.word_suggestions(query)
        if not query:
            return []
        item_index = self.item_dict[query[0]]
        # update appearance count
        self.add_appearance(item_index)
        results = self._recommend(item_index, limit)
        return self._extend_results(results, item_index, limit)
            
    def _recommend(self, item_index: int, limit: int) -> list[int]:
        """Returns recommendations for an item index, up to limit."""
        # use precomputed paths if available
        results = []
        try:
            rec_path = os.path.join(self.pc_path, f'{item_index}.pkl')
            with open(rec_path, 'rb') as f:
                item_indices = pickle.load(f)
            for cnt, i in enumerate(item_indices):
                if cnt > limit:
                    break
                if i == item_index:
                    continue
                results.append(i)
        except:   
            # run algorithm otherwise
            results_heap = dijkstra(self, item_index)
            for cnt, i in enumerate(results_heap):
                if cnt > limit:
                    break
                if i == item_index:
                    continue
                results.append(i)
        return results
    
    def latest(self, limit: int=10) -> list[SearchItem]:
        """Returns a list of the newest-added SearchItems."""
        results = []
        for i in range(len(self)-1, len(self)-limit-1, -1):
            try:
                results.append(self.items[i])
            except:
                break
        return results
    
    def trending(self, limit: int=10) -> list[SearchItem]:
        """Returns a list of the highest-view count SearchItems."""
        results = []
        popped = []
        for _ in range(limit):
            item_index = self.interests.pop()
            item = self.get_item_by_index(item_index)
            popped.append((item_index, item.get_interest()))
            results.append(item)
        while len(popped):
            item_index, item_interest = popped.pop()
            self.interests.add(item_index, item_interest)
        return results
    
    def _extend_results(self, results: list[int],
                        item_index: int, 
                        limit: int) -> list[SearchItem]:
        """Extends the results until limit is reached."""
        # return if limit reached      
        if len(results) > limit:
            return [self.get_item_by_index(i) for i in results]
        # add to results until limit
        name = self.get_item_by_index(item_index).get_name()
        for i in range(len(results)):
            if i > limit:
                break
            result = self.get_item_by_index(i)
            # likely not same series, find more recommendations
            if name_similarity(name, result.get_name()) <= 0.5:
                for j in self._recommend(item_index, limit-len(results)):
                    if len(results) > limit:
                        break
                    results.append(j)
                    
        return [self.get_item_by_index(k) for k in results] 
    
    def parse_results(self, heap: MinHeap, limit: int) -> list[SearchItem]:
        """Returns a list of search results from a MinHeap
        of SearchItems and an int limit."""
        results = []
        for i, item_index in enumerate(heap):
            if i > limit:
                break
            results.append(self.get_item_by_index(item_index))
        return results
    
    def save_all_recommends(self, dir_path: str, 
                            start_index: int=0, 
                            end_index: int=None, 
                            p_count: int=cpu_count()) -> FileIO:
        """Saves all recommendations into a given filepath. 
        
        This function uses multiprocessing and must be run 
        inside a __main__ method.
        
        Args:
            dir_path: A str directory path to store all results.
            start_index: An (optional) int item index to start at.
                Defaults to 0.
            end_index: An (optional) int item index to end at (exclusive).
                Defaults to max index.
            p_count: An (optional) int number of processors to run. 
                Defaults to max available processors.
        """
        size = len(self.items)
        if end_index is None:
            end_index = size
        # create directory if it doesn't exist
        if not os.path.isdir(dir_path):
            os.mkdir(dir_path)
        print(f'[STATUS] save_all_recommends(): '
              f'Computing {end_index-start_index} '
              f'recommendations using {p_count} processors.')
        t0 = time()
        # save all recommendations by index
        self._rec_path = dir_path
        with Pool(processes=p_count) as pool:
            pool.map(self._save_recommend, 
                     (i for i in range(start_index, end_index)))
        print(f'[STATUS] save_all_recommends(): '
              f'Saving {end_index-start_index} recommendations to {dir_path}.\n'
              f'   > Finished in {time()-t0} seconds.')
            
    def _save_recommend(self, item_index: int) -> FileIO:
        """Returns a list of top 100 results from 
        a tuple of item index and dir path."""
        item_index, dir_path = item_index, self._rec_path
        results = self.parse_results(dijkstra(self, item_index), 100)
        item_indices = []
        for item in results:
            wtf_name = wordtrie_format(item.get_name())
            item_indices.append(self.item_dict[wtf_name])
        file_path = os.path.join(dir_path, f'{item_index}.pkl')
        with open(file_path, 'wb') as f:
            pickle.dump(item_indices, f)
